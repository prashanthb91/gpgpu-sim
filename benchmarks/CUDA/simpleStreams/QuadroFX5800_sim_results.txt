

        *** GPGPU-Sim version 2.1.1b (beta) ***


GPGPU-Sim PTX: __cudaRegisterFunction _Z10init_arrayPiS_i : 0x404010
[ simpleStreams ]
GPGPU-Sim PTX: simulation mode 0 (can change with PTX_SIM_MODE_FUNC environment variable:
               1=functional simulation only, 0=detailed performance simulator)
GPGPU-Sim: Configuration options:

-network_mode                           1 # Interconnection network mode
-inter_config_file   icnt_config_quadro_islip.txt # Interconnection network config file
-save_embedded_ptx                      0 # saves ptx files embedded in binary as <n>.ptx
-gpgpu_simd_model                       1 # 0 = no recombination, 1 = post-dominator, 2 = MIMD, 3 = dynamic warp formation
-gpgpu_dram_scheduler                    1 # 0 = fifo (default), 1 = fast ideal
-gpgpu_max_cycle                        0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_insn                         0 # terminates gpu simulation early (0 = no limit)
-gpgpu_tex_cache:l1             64:64:2:L # per-shader L1 texture cache  (READ-ONLY) config, i.e., {<nsets>:<linesize>:<assoc>:<repl>|none}
-gpgpu_const_cache:l1            64:64:2:L # per-shader L1 constant memory cache  (READ-ONLY) config, i.e., {<nsets>:<linesize>:<assoc>:<repl>|none}
-gpgpu_no_dl1                           1 # no dl1 cache (voids -gpgpu_cache:dl1 option)
-gpgpu_cache:dl1               128:64:4:L # shader L1 data cache config, i.e., {<nsets>:<bsize>:<assoc>:<repl>|none}
-gpgpu_cache:dl2                     NULL # unified banked L2 data cache config, i.e., {<nsets>:<bsize>:<assoc>:<repl>|none}; disabled by default
-gpgpu_perfect_mem                      0 # enable perfect memory mode (no cache miss)
-gpgpu_shader_core_pipeline           1024:32:32 # shader core pipeline config, i.e., {<nthread>:<warpsize>:<pipe_simd_width>}
-gpgpu_shader_registers                16384 # Number of registers per shader core. Limits number of concurrent CTAs. (default 8192)
-gpgpu_shader_cta                       8 # Maximum number of concurrent CTAs in shader (default 8)
-gpgpu_n_shader                        30 # number of shaders in gpu
-gpgpu_n_mem                            8 # number of memory modules (e.g. memory controllers) in gpu
-gpgpu_n_mem_per_ctrlr                    2 # number of memory chips per memory controller
-gpgpu_runtime_stat                  1000 # display runtime statistics such as dram utilization {<freq>:<flag>}
-gpgpu_dwf_heuristic                    0 # DWF scheduling heuristic: 0 = majority, 1 = minority, 2 = timestamp, 3 = pdom priority, 4 = pc-based, 5 = max-heap
-gpgpu_reg_bankconflict                    0 # Check for bank conflict in the pipeline
-gpgpu_dwf_regbk                        1 # Have dwf scheduler to avoid bank conflict
-gpgpu_memlatency_stat                   14 # track and display latency statistics 0x2 enables MC, 0x4 enables queue logs
-gpgpu_mshr_per_thread                    1 # Number of MSHRs per thread
-gpgpu_interwarp_mshr_merge                    6 # interwarp coalescing
-gpgpu_dram_sched_queue_size                   32 # 0 = unlimited (default); # entries per chip
-gpgpu_dram_buswidth                    4 # default = 4 bytes (8 bytes per cycle at DDR)
-gpgpu_dram_burst_length                    4 # Burst length of each DRAM request (default = 4 DDR cycle)
-gpgpu_dram_timing_opt 8:2:8:12:25:10:35:10:7:6 # DRAM timing parameters = {nbk:tCCD:tRRD:tRCD:tRAS:tRP:tRC:CL:WL:tWTR}
-gpgpu_mem_address_mask                    1 # 0 = old addressing mask, 1 = new addressing mask, 2 = new add. mask + flipped bank sel and chip sel bits
-gpgpu_flush_cache                      0 # Flush cache at the end of each kernel call
-gpgpu_l2_readoverwrite                    0 # Prioritize read requests over write requests for L2
-gpgpu_pre_mem_stages                    1 # default = 0 pre-memory pipeline stages
-gpgpu_no_divg_load                     0 # Don't allow divergence on load
-gpgpu_dwf_hw                        32:2 # dynamic warp formation hw config, i.e., {<#LUT_entries>:<associativity>|none}
-gpgpu_thread_swizzling                    0 # Thread Swizzling (1=on, 0=off)
-gpgpu_strict_simd_wrbk                    0 # Applying Strick SIMD WriteBack Stage (1=on, 0=off)
-gpgpu_shmem_size                   16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_bkconflict                    1 # Turn on bank conflict check for shared memory
-gpgpu_shmem_pipe_speedup                    1 # Number of groups each warp is divided for shared memory bank conflict check
-gpgpu_L2_queue         0:0:0:0:0:0:10:10 # L2 data cache queue length and latency config
-gpgpu_cache_wt_through                    0 # L1 cache become write through (1=on, 0=off)
-gpgpu_deadlock_detect                    1 # Stop the simulation at deadlock (1=on (default), 0=off)
-gpgpu_cache_bkconflict                    1 # Turn on bank conflict check for L1 cache access
-gpgpu_n_cache_bank                     1 # Number of banks in L1 cache, also for memory coalescing stall
-gpgpu_warpdistro_shader                   -1 # Specify which shader core to collect the warp size distribution from
-gpgpu_pdom_sched_type                    8 # 0 = first ready warp found, 1 = random, 8 = loose round robin
-gpgpu_spread_blocks_across_cores                    1 # Spread block-issuing across all cores instead of filling up core by core
-gpgpu_cuda_sim                         1 # use PTX instruction set
-gpgpu_ptx_instruction_classification                    0 # if enabled will classify ptx instruction types per kernel (Max 255 kernels now)
-gpgpu_ptx_sim_mode                     0 # Select between Performance (default) or Functional simulation (1)
-gpgpu_clock_domains 325.0:650.0:650.0:800.0 # Clock Domain Frequencies in MhZ {<Core Clock>:<ICNT Clock>:<L2 Clock>:<DRAM Clock>}
-gpgpu_shmem_port_per_bank                    2 # Number of access processed by a shared memory bank per cycle (default = 2)
-gpgpu_cache_port_per_bank                    2 # Number of access processed by a cache bank per cycle (default = 2)
-gpgpu_const_port_per_bank                    2 # Number of access processed by a constant cache bank per cycle (default = 2)
-gpgpu_cflog_interval                    0 # Interval between each snapshot in control flow logger
-gpgpu_partial_write_mask                    1 # use partial write mask to filter memory requests <1>No extra reads(use this!)<2>extra reads generated for partial chunks
-gpu_concentration                      3 # Number of shader cores per interconnection port (default = 1)
-gpgpu_local_mem_map                    1 # Mapping from local memory space address to simulated GPU physical address space (default = 1)
-pipetrace_out              pipetrace.txt # specifies file to output pipetrace to
-pipetrace                              0 # Turns pipetrace on/off
-gpgpu_mem_addr_mapping dramid@8;00000000.00000000.00000000.00000000.0000RRRR.RRRRRRRR.RRBBBCCC.CCCSSSSS # mapping memory address to dram model {dramid@<start bit>;<memory address map>}
-visualizer_enabled                     1 # Turn on visualizer output (1=On, 0=Off)
-visualizer_outputfile                 NULL # Specifies the output log file for visualizer
-visualizer_zlevel                      6 # Compression level of the visualizer output log (0=no comp, 9=highest)
-enable_ptx_file_line_stats                    1 # Turn on PTX source line statistic profiling. (1 = On)
-ptx_line_stats_filename gpgpu_inst_stats.txt # Output file for PTX source line statistics.
32 30 1024 983040
30720
addr_dec_mask[CHIP]  = 0000000000000700 	high:11 low:8
addr_dec_mask[BK]    = 000000000001c000 	high:17 low:14
addr_dec_mask[ROW]   = 000000007ffe0000 	high:31 low:17
addr_dec_mask[COL]   = 00000000000038ff 	high:14 low:0
addr_dec_mask[BURST] = 000000000000001f 	high:5 low:0
*** Initializing Memory Statistics ***
nodemap
0 
1 2 3 4 
5 6 7 8 
9 10 11 12 
13 14 15 16 
17 GPU performance model initialization complete.
GPGPU-Sim PTX: USING EMBEDDED .ptx files...
GPGPU-Sim PTX: parsing function _Z10init_arrayPiS_i
GPGPU-Sim PTX: instruction assembly for function '_Z10init_arrayPiS_i'...   done.
GPGPU-Sim PTX: Finding postdominators for '_Z10init_arrayPiS_i'...
GPGPU-Sim PTX: Finding immediate postdominators for '_Z10init_arrayPiS_i'...
GPGPU-Sim PTX: finished parsing EMBEDDED .ptx file _1.ptx
GPGPU-Sim PTX: extracting embedded .ptx to temporary file "_ptx_LE8TX8"
GPGPU-Sim PTX: generating ptxinfo using "ptxas -v _ptx2_aZB0rC --output-file /dev/null 2> _ptx_LE8TX8info"
GPGPU-Sim PTX: Kernel _Z10init_arrayPiS_i
GPGPU-Sim PTX: removing ptxinfo using "rm -f _ptx_LE8TX8 _ptx2_aZB0rC _ptx_LE8TX8info"
GPGPU-Sim PTX: loading global with explicit initializers...  done.
GPGPU-Sim PTX: loading constants with explicit initializers...  done.
GPGPU-Sim PTX: Program parsing completed (max 28 registers used per thread).
> Device name : GPGPU-Sim_v2.1.1b (beta)
> CUDA Capable SM 1.3 hardware with 30 multi-processors
> scale_factor = 1.0000
> array_size   = 16777216

GPGPU-Sim PTX: allocating 67108864 bytes on GPU starting at address 0x10000000
GPGPU-Sim PTX: cudaMallocing 67108864 bytes starting at 0x10000000..
GPGPU-Sim PTX: allocating 4 bytes on GPU starting at address 0x14000000
GPGPU-Sim PTX: cudaMallocing 4 bytes starting at 0x14000000..
GPGPU-Sim PTX: cudaMemcpy(): devPtr = 0x14000000
GPGPU-Sim PTX: copying 4 bytes from CPU[0x7fff06f7635c] to GPU[0x14000000] ...  done.


GPGPU-Sim PTX: Execution error: CUDA API function "cudaError_t cudaStreamCreate(cudaStream_t*)()" has not been implemented yet.
                 [$GPGPUSIM_ROOT/libcuda/cuda_runtime_api.cc around line 855]


